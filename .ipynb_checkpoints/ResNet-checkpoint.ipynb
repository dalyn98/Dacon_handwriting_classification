{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5907fe54",
   "metadata": {},
   "source": [
    "### Resnet\n",
    "\n",
    "레이어가 많아져 신경망이 깊어질수록 기울기 소실/ 폭발 문제가 커진다  \n",
    "깊은 레이어까지 학습이 잘되도록 하는 방법이 shortcut(Skip) Connection 입니다\n",
    "\n",
    "low-level vision과 computer graphics 분야에서 Partial Differential Equations(PDEs)를 풀기 위해   \n",
    "Multigrid 방법들이 많이 사용됩니다.  \n",
    "이것은 다양한 스케일(크기)에서의 하위문제로 시스템을 재정의하는 것입니다.    \n",
    "쉽게 설명하면, 코카콜라 캔을 학습한다고 하였을 때, 코카콜라 캔이 가까이에서 찍힌 것은 크게 나타나고, 멀리서 찍힌 것은 작게 나타납니다.   \n",
    "이러한 스케일의 변화에 대응하기 위한 컴퓨터 비전 기술은 pyramid와 같은 형태의 multigrid를 만들어서 해결합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c79187",
   "metadata": {},
   "source": [
    "그런데 다른점은 무엇이냐면, 이전에는 multigrid를 만들어서 (이미지 크기를 다양하게 하거나, 필터 크기를 다양하게 해서) 각각을 계산하는 방식을 사용하였습니다.   \n",
    "기존의 CNN 구조에서는 이전의 것이 다음으로 전달되어 영향을 미치게됩니다.   \n",
    "입력부분에 가까운 하위 레이어에서는 매우 단순한 구조나 노이지한 패턴이 보이는 low-level feature가 학습이되고, 출력부분에 가까운 상위 레이어에서는 구조적인 부분이 학습되는 high-level feature가 학습됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419ce7c",
   "metadata": {},
   "source": [
    "그런데 앞선 부분의 feature가 뒤쪽까지 영향이 직접적으로 전달되는 것이 아니라, 중간을 거쳐 전달되기 때문에 학습의 과정에서 크게크게 변합니다.  \n",
    "그런데 shortcut connection을 추가해주게 되면 (수식적으로) 이전으로부터 얼만큼 변하는지 나머지(residual)만 계산하는 문제로 바뀌게 됩니다.  \n",
    "즉, 현재 레이어의 출력값과 이전 스케일의 레이어 출력값을 더해 입력을 받기 때문에,   \n",
    "그 차이를 볼 수 있게 되는 것이죠.  \n",
    "따라서 학습하는 과정에서 그 '조금'을 하면 되는 것이고, 더 빠르게 학습한다는 장점이 생깁니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b088c",
   "metadata": {},
   "source": [
    "이전에 얕은 모델과 깊은 모델을 비교했을 때, 깊은 모델이 더 안좋아진다고 했었습니다.   \n",
    "그런데 Kaiming He는 그래서는 안된다고 했습니다.  \n",
    "그런 문제점을 identity mapping이라는 것을 통해 꼬집었습니다.   \n",
    "얕은 모델에서 단순히 아무것도 하지 않는 layer인(convolution을 통과하지 않고 값을 전달하는)  \n",
    "identity mapping을 쌓으면(덧셈 연산으로)   \n",
    "얕은 모델 그대로의 성능을 나타낼 것이라는 자명한 사실에 하나의 가정을 더합니다.  \n",
    "\"쌓여있는 레이어가 underlying mapping을 fit하는 것보다 residual mppaing을 fit하는 것이 쉽다.\"   \n",
    "그리고 shortcut connection이 이 역할을 정확하게 할 수 있다고 말합니다.   \n",
    "즉, 이전에 학습된 모델(레이어들)의 출력과 추가된 레이어의 출력의 차이값인 나머지(residual)만 학습하면 되기에 연산이 간단해지고, error값 크기의 측면에서 학습이 더 쉽다는 것입니다.\n",
    "\n",
    "identity mapping은 입력값을 그대로 전달한다는 의미에서 identity입니다.  \n",
    "위의 shortcut connection에서 identity로 표현한 것처럼,   \n",
    "shortcut connection과 identity mapping은 다른 것이 아니라,   \n",
    "의미적으로 identity는 값을 그대로 보낸다는 것입니다. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84e087",
   "metadata": {},
   "source": [
    "1. 이미지에서는 H(x) = x가 되도록 학습시킨다.\n",
    "\n",
    "2. 네트워크의 output F(x)는 0이 되도록 학습시킨다.\n",
    "\n",
    "3. F(x)+x=H(x)=x가 되도록 학습시키면 미분해도 F(x)+x의 미분값은 F'(x) + 1로 최소 1이상이다.\n",
    "\n",
    "4. 모든 layer에서의 gradient가 1+F'(x)이므로 gradient vanishing현상을 해결했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86df33eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 처음을 제외하고는 균일하게 3 x 3 사이즈의 컨볼루션 필터를 사용했다\\n그리고 특성맵의 사이즈가 반으로 줄어들 때 특성맵의 뎁스를 2배로 높임'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' 처음을 제외하고는 균일하게 3 x 3 사이즈의 컨볼루션 필터를 사용했다\n",
    "그리고 특성맵의 사이즈가 반으로 줄어들 때 특성맵의 뎁스를 2배로 높임'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f9ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, random, time\n",
    "import copy\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83faa0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filen_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train0001.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train0002.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train0003.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train0004.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train0005.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>train4996.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>train4997.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>train4998.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>train4999.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>train5000.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         filen_name  label\n",
       "0     train0001.png      8\n",
       "1     train0002.png      8\n",
       "2     train0003.png      8\n",
       "3     train0004.png      8\n",
       "4     train0005.png      8\n",
       "...             ...    ...\n",
       "4995  train4996.png      6\n",
       "4996  train4997.png      6\n",
       "4997  train4998.png      6\n",
       "4998  train4999.png      6\n",
       "4999  train5000.png      6\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './train/train_data.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f69b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = df['filen_name']\n",
    "train_label = df['label']\n",
    "\n",
    "# image 파일을 불러온뒤 변수에 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a0ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_csv = pd.read_csv(os.path.join('./', \"train\",\"train_data.csv\"))\n",
    "tr_csv['path'] = tr_csv['filen_name'].apply(\n",
    "    lambda x: os.path.join('train',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da98fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "    tr_csv['path'].values, tr_csv[\"label\"].values, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6008718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(true, pred):\n",
    "    return sum(true == pred) / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec0a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.OneOf([A.Rotate(limit=10),\n",
    "                 A.RandomBrightness(),\n",
    "                 A.CoarseDropout(),\n",
    "                 A.Cutout(num_holes=8, max_h_size=1, max_w_size=1, fill_value=1),\n",
    "                 ], p=1.0),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n",
    "def get_valid_transforms():\n",
    "    return ToTensorV2(p=1.0)\n",
    "\n",
    "def get_inferecne_transforms():\n",
    "    return ToTensorV2(p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9d95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self,X=None, y=None,transforms = None):\n",
    "            super().__init__()\n",
    "            self.file_path_list = X\n",
    "            self.labels = y \n",
    "            self.transforms = transforms\n",
    "    def __getitem__(self,idx):\n",
    "        image = Image.open(self.file_path_list[idx]).convert(\"RGB\")\n",
    "        image = np.array(image, dtype = np.float32)\n",
    "        image /= 255\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            label = torch.tensor(label,dtype = torch.int64)\n",
    "            return image, label\n",
    "         \n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f2f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0af4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(Dataset):\n",
    "    def __init__(self, X=None, y=None, transforms=None):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.X[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img /= 255\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        if self.y is not None:\n",
    "            label = self.y[idx]\n",
    "            label = torch.tensor(label, dtype=torch.int64)\n",
    "            return img, label\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b12d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST(train_x,train_y,get_train_transforms())\n",
    "test_data = MNIST(valid_x,valid_y,get_valid_transforms())\n",
    "train_dl = DataLoader(train_data,batch_size =32, shuffle = True, num_workers = NUM_CPU)\n",
    "test_dl = DataLoader(test_data,batch_size =32, shuffle = False, num_workers = NUM_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e5c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac61fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d96b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,in_channels, out_channels, stride = 1):\n",
    "        super().__init__()\n",
    "        #Batch Norm에 bias가 포함되어 있음으로 conv2d는 bias=Flase로 설정\n",
    "        \n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride=stride,padding =1,bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels,out_channels * BasicBlock.expansion, kernel_size =3 ,stride = stride, padding= 1 ,bias = False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels :\n",
    "            self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size = 1, stride= stride, bias = False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f6bf595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,block,num_block,num_classes = 10, init_weights = True):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "        nn.Conv2d(3,64,kernel_size = 7, stride = 2,padding =3 ,bias =False),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 3, stride =2, padding =1)\n",
    "        )\n",
    "        self.conv2_x = self._make_layer(block,64,num_block[0],1)\n",
    "        self.conv3_x = self._make_layer(block,128,num_block[1],1)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "            \n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks -1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels,out_channels,stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        x = self.conv2_x(output)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode = 'fan_out',nonlinearity ='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "                elif isinstance(m,nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight,1)\n",
    "                    nn.init.constant_(m.bias,0)\n",
    "                elif isinstance(m,nn.Linear):\n",
    "                    nn.init.normal_(m.weight,0,0.01)\n",
    "                    nn.init.constatnt_(m.bais,0)\n",
    "def resnet18():\n",
    "        return Net(BasicBlock,[2,2])\n",
    "def resnet34():\n",
    "        return Net(BasicBlock,[3,4,6,3])\n",
    "net = resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f263a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet34(pretrained = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45c3f596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 14, 14]             128\n",
      "              ReLU-3           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 7, 7]             128\n",
      "              ReLU-7             [-1, 64, 7, 7]               0\n",
      "            Conv2d-8             [-1, 64, 7, 7]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 7, 7]             128\n",
      "             ReLU-10             [-1, 64, 7, 7]               0\n",
      "       BasicBlock-11             [-1, 64, 7, 7]               0\n",
      "           Conv2d-12             [-1, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 7, 7]             128\n",
      "             ReLU-14             [-1, 64, 7, 7]               0\n",
      "           Conv2d-15             [-1, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 7, 7]             128\n",
      "             ReLU-17             [-1, 64, 7, 7]               0\n",
      "       BasicBlock-18             [-1, 64, 7, 7]               0\n",
      "           Conv2d-19             [-1, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-20             [-1, 64, 7, 7]             128\n",
      "             ReLU-21             [-1, 64, 7, 7]               0\n",
      "           Conv2d-22             [-1, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-23             [-1, 64, 7, 7]             128\n",
      "             ReLU-24             [-1, 64, 7, 7]               0\n",
      "       BasicBlock-25             [-1, 64, 7, 7]               0\n",
      "           Conv2d-26            [-1, 128, 7, 7]          73,728\n",
      "      BatchNorm2d-27            [-1, 128, 7, 7]             256\n",
      "             ReLU-28            [-1, 128, 7, 7]               0\n",
      "           Conv2d-29            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-30            [-1, 128, 7, 7]             256\n",
      "           Conv2d-31            [-1, 128, 7, 7]           8,192\n",
      "      BatchNorm2d-32            [-1, 128, 7, 7]             256\n",
      "             ReLU-33            [-1, 128, 7, 7]               0\n",
      "       BasicBlock-34            [-1, 128, 7, 7]               0\n",
      "           Conv2d-35            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-36            [-1, 128, 7, 7]             256\n",
      "             ReLU-37            [-1, 128, 7, 7]               0\n",
      "           Conv2d-38            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-39            [-1, 128, 7, 7]             256\n",
      "             ReLU-40            [-1, 128, 7, 7]               0\n",
      "       BasicBlock-41            [-1, 128, 7, 7]               0\n",
      "           Conv2d-42            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-43            [-1, 128, 7, 7]             256\n",
      "             ReLU-44            [-1, 128, 7, 7]               0\n",
      "           Conv2d-45            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-46            [-1, 128, 7, 7]             256\n",
      "             ReLU-47            [-1, 128, 7, 7]               0\n",
      "       BasicBlock-48            [-1, 128, 7, 7]               0\n",
      "           Conv2d-49            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-50            [-1, 128, 7, 7]             256\n",
      "             ReLU-51            [-1, 128, 7, 7]               0\n",
      "           Conv2d-52            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-53            [-1, 128, 7, 7]             256\n",
      "             ReLU-54            [-1, 128, 7, 7]               0\n",
      "       BasicBlock-55            [-1, 128, 7, 7]               0\n",
      "AdaptiveAvgPool2d-56            [-1, 128, 1, 1]               0\n",
      "           Linear-57                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 1,349,194\n",
      "Trainable params: 1,349,194\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.25\n",
      "Params size (MB): 5.15\n",
      "Estimated Total Size (MB): 7.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "net.to(device)\n",
    "summary(net, (3,28,28), device = device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b5e55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations==1.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from albumentations==1.0.3) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from albumentations==1.0.3) (1.19.5)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from albumentations==1.0.3) (0.18.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from albumentations==1.0.3) (1.6.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\users\\현우\\appdata\\roaming\\python\\python38\\site-packages (from albumentations==1.0.3) (4.5.4.60)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (3.3.4)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.5)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (8.2.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (2021.4.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations==1.0.3) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.3) (1.15.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==1.0.3) (5.0.6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# *------- torch -------*\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "# *------- albumentations -------*\n",
    "!pip install albumentations==1.0.3\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# *------- sklearn -------*\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc77d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (28,28)\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 50\n",
    "NUM_CPU = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa675091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [64, 64, 14, 14]           9,408\n",
      "       BatchNorm2d-2           [64, 64, 14, 14]             128\n",
      "              ReLU-3           [64, 64, 14, 14]               0\n",
      "         MaxPool2d-4             [64, 64, 7, 7]               0\n",
      "            Conv2d-5             [64, 64, 7, 7]          36,864\n",
      "       BatchNorm2d-6             [64, 64, 7, 7]             128\n",
      "              ReLU-7             [64, 64, 7, 7]               0\n",
      "            Conv2d-8             [64, 64, 7, 7]          36,864\n",
      "       BatchNorm2d-9             [64, 64, 7, 7]             128\n",
      "             ReLU-10             [64, 64, 7, 7]               0\n",
      "       BasicBlock-11             [64, 64, 7, 7]               0\n",
      "           Conv2d-12             [64, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-13             [64, 64, 7, 7]             128\n",
      "             ReLU-14             [64, 64, 7, 7]               0\n",
      "           Conv2d-15             [64, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-16             [64, 64, 7, 7]             128\n",
      "             ReLU-17             [64, 64, 7, 7]               0\n",
      "       BasicBlock-18             [64, 64, 7, 7]               0\n",
      "           Conv2d-19             [64, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-20             [64, 64, 7, 7]             128\n",
      "             ReLU-21             [64, 64, 7, 7]               0\n",
      "           Conv2d-22             [64, 64, 7, 7]          36,864\n",
      "      BatchNorm2d-23             [64, 64, 7, 7]             128\n",
      "             ReLU-24             [64, 64, 7, 7]               0\n",
      "       BasicBlock-25             [64, 64, 7, 7]               0\n",
      "           Conv2d-26            [64, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-27            [64, 128, 4, 4]             256\n",
      "             ReLU-28            [64, 128, 4, 4]               0\n",
      "           Conv2d-29            [64, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-30            [64, 128, 4, 4]             256\n",
      "           Conv2d-31            [64, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-32            [64, 128, 4, 4]             256\n",
      "             ReLU-33            [64, 128, 4, 4]               0\n",
      "       BasicBlock-34            [64, 128, 4, 4]               0\n",
      "           Conv2d-35            [64, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-36            [64, 128, 4, 4]             256\n",
      "             ReLU-37            [64, 128, 4, 4]               0\n",
      "           Conv2d-38            [64, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-39            [64, 128, 4, 4]             256\n",
      "             ReLU-40            [64, 128, 4, 4]               0\n",
      "       BasicBlock-41            [64, 128, 4, 4]               0\n",
      "           Conv2d-42            [64, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-43            [64, 128, 4, 4]             256\n",
      "             ReLU-44            [64, 128, 4, 4]               0\n",
      "           Conv2d-45            [64, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-46            [64, 128, 4, 4]             256\n",
      "             ReLU-47            [64, 128, 4, 4]               0\n",
      "       BasicBlock-48            [64, 128, 4, 4]               0\n",
      "           Conv2d-49            [64, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-50            [64, 128, 4, 4]             256\n",
      "             ReLU-51            [64, 128, 4, 4]               0\n",
      "           Conv2d-52            [64, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53            [64, 128, 4, 4]             256\n",
      "             ReLU-54            [64, 128, 4, 4]               0\n",
      "       BasicBlock-55            [64, 128, 4, 4]               0\n",
      "           Conv2d-56            [64, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-57            [64, 256, 2, 2]             512\n",
      "             ReLU-58            [64, 256, 2, 2]               0\n",
      "           Conv2d-59            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-60            [64, 256, 2, 2]             512\n",
      "           Conv2d-61            [64, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-62            [64, 256, 2, 2]             512\n",
      "             ReLU-63            [64, 256, 2, 2]               0\n",
      "       BasicBlock-64            [64, 256, 2, 2]               0\n",
      "           Conv2d-65            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-66            [64, 256, 2, 2]             512\n",
      "             ReLU-67            [64, 256, 2, 2]               0\n",
      "           Conv2d-68            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-69            [64, 256, 2, 2]             512\n",
      "             ReLU-70            [64, 256, 2, 2]               0\n",
      "       BasicBlock-71            [64, 256, 2, 2]               0\n",
      "           Conv2d-72            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-73            [64, 256, 2, 2]             512\n",
      "             ReLU-74            [64, 256, 2, 2]               0\n",
      "           Conv2d-75            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-76            [64, 256, 2, 2]             512\n",
      "             ReLU-77            [64, 256, 2, 2]               0\n",
      "       BasicBlock-78            [64, 256, 2, 2]               0\n",
      "           Conv2d-79            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-80            [64, 256, 2, 2]             512\n",
      "             ReLU-81            [64, 256, 2, 2]               0\n",
      "           Conv2d-82            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-83            [64, 256, 2, 2]             512\n",
      "             ReLU-84            [64, 256, 2, 2]               0\n",
      "       BasicBlock-85            [64, 256, 2, 2]               0\n",
      "           Conv2d-86            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-87            [64, 256, 2, 2]             512\n",
      "             ReLU-88            [64, 256, 2, 2]               0\n",
      "           Conv2d-89            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-90            [64, 256, 2, 2]             512\n",
      "             ReLU-91            [64, 256, 2, 2]               0\n",
      "       BasicBlock-92            [64, 256, 2, 2]               0\n",
      "           Conv2d-93            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-94            [64, 256, 2, 2]             512\n",
      "             ReLU-95            [64, 256, 2, 2]               0\n",
      "           Conv2d-96            [64, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-97            [64, 256, 2, 2]             512\n",
      "             ReLU-98            [64, 256, 2, 2]               0\n",
      "       BasicBlock-99            [64, 256, 2, 2]               0\n",
      "          Conv2d-100            [64, 512, 1, 1]       1,179,648\n",
      "     BatchNorm2d-101            [64, 512, 1, 1]           1,024\n",
      "            ReLU-102            [64, 512, 1, 1]               0\n",
      "          Conv2d-103            [64, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-104            [64, 512, 1, 1]           1,024\n",
      "          Conv2d-105            [64, 512, 1, 1]         131,072\n",
      "     BatchNorm2d-106            [64, 512, 1, 1]           1,024\n",
      "            ReLU-107            [64, 512, 1, 1]               0\n",
      "      BasicBlock-108            [64, 512, 1, 1]               0\n",
      "          Conv2d-109            [64, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-110            [64, 512, 1, 1]           1,024\n",
      "            ReLU-111            [64, 512, 1, 1]               0\n",
      "          Conv2d-112            [64, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-113            [64, 512, 1, 1]           1,024\n",
      "            ReLU-114            [64, 512, 1, 1]               0\n",
      "      BasicBlock-115            [64, 512, 1, 1]               0\n",
      "          Conv2d-116            [64, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-117            [64, 512, 1, 1]           1,024\n",
      "            ReLU-118            [64, 512, 1, 1]               0\n",
      "          Conv2d-119            [64, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-120            [64, 512, 1, 1]           1,024\n",
      "            ReLU-121            [64, 512, 1, 1]               0\n",
      "      BasicBlock-122            [64, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-123            [64, 512, 1, 1]               0\n",
      "          Linear-124                   [64, 10]           5,130\n",
      "================================================================\n",
      "Total params: 21,289,802\n",
      "Trainable params: 21,289,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 110.07\n",
      "Params size (MB): 81.21\n",
      "Estimated Total Size (MB): 191.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 사전학습된 가중치를 가져오지 않도록 pretrained는 Fasle\n",
    "model = torchvision.models.resnet34(pretrained = False)\n",
    "\n",
    "# number of features in the input of the linear layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# sets the number of features of the linear layer\n",
    "model.fc = torch.nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "\n",
    "# parameters\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "model = model.to(device)\n",
    "\n",
    "# model summary\n",
    "summary(model, (3, IMG_SIZE[0], IMG_SIZE[1]), BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f444a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs, train_loader,val_loader):\n",
    "    since = time.time()\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        \n",
    "        for step, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / val_size\n",
    "        epoch_acc = running_corrects.double() / val_size\n",
    "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        print('-' * 30)\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val Acc: {:.4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44924920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "model = net.to(device)\n",
    "param = list(net.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2d80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, 50, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c23d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for Epoch in tqdm(range(30)):\n",
    "    for batch, labels in train_dl:\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch)\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = compute_acc(labels.detach().cpu().numpy(), output.detach().cpu().numpy().argmax(-1))\n",
    "        \n",
    "    if Epoch % 10 == 0 or Epoch == 29:\n",
    "        print(f'Epoch {Epoch}, loss : {loss}, acc : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eeb7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test/test_data.csv') \n",
    "test_file_dir = './test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6c34122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 157/157 [00:05<00:00, 26.78it/s]\n"
     ]
    }
   ],
   "source": [
    "test_mnist_dataset = MNIST(test_file_dir + test_df['file_name'])\n",
    "test_mnist_loader = DataLoader(test_mnist_dataset, batch_size = 32)\n",
    "preds = None\n",
    "\n",
    "for test_batch in tqdm(test_mnist_loader):\n",
    "    test_batch = test_batch.to(device)\n",
    "    output = net(test_batch)\n",
    "    \n",
    "    digit_pred = output.detach().cpu().numpy().argmax(-1)\n",
    "    if preds is None:\n",
    "        preds = digit_pred\n",
    "    else:\n",
    "        preds = np.concatenate([preds,digit_pred])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3c1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv') # sample submission 불러오기\n",
    "\n",
    "submission['label'] = preds\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bd88ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[4866]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055447ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
