{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51849679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# *------- Basic setup -------*\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# *------- torch -------*\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "#import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "# *------- albumentations -------*\n",
    "#!pip install albumentations==1.0.3\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# *------- sklearn -------*\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# *------- path -------*\n",
    "base_path = \"./\"\n",
    "\n",
    "# test 폴더를 한 단계 상위 폴더로 옮겨서 사용했습니다.\n",
    "# 옮기지 않은 경우\n",
    "#test_path = os.path.join(base_path, \"test\",\"test\")\n",
    "test_path = os.path.join(base_path, \"test\")\n",
    "train_path = os.path.join(base_path, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fad123",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "algumentation : https://dacon.io/competitions/official/235838/codeshare/3734?page=1&dtype=recent\n",
    "pseudo_labeling : https://github.com/anirudhshenoy/pseudo_labeling_small_datasets/blob/master/pseudo_label-DL.ipynb\n",
    "Model : https://deep-learning-study.tistory.com/563\n",
    "LrSceduler,Early Stopping : https://dacon.io/competitions/official/235838/codeshare/3778?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22abd18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4fec4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1010)\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "IMG_SIZE = (28,28)\n",
    "BATCH_SIZE = 32\n",
    "ULBATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d0b18a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.OneOf([A.Rotate(limit=25,interpolation=cv2.INTER_NEAREST),\n",
    "                 A.RandomResizedCrop(height=28,width=28,scale=(.35, 1.),ratio=(.5, 2),\n",
    "                        interpolation=cv2.INTER_NEAREST)]),\n",
    "    ToTensorV2(p=1.0),])\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([ToTensorV2(p=1.0)])\n",
    "\n",
    "def get_inferecne_transforms():\n",
    "    return ToTensorV2(p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "38003216",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, X=None, y=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.X[idx]\n",
    "        image = Image.open(img_path)#.convert(\"RGB\")\n",
    "        image = np.array(image, dtype='float32')\n",
    "        #img /= 255\n",
    "        image = self.transform(image = image)['image']\n",
    "        \n",
    "        if self.y is not None:\n",
    "            label = self.y[idx]\n",
    "            label = torch.tensor(label, dtype=torch.int64)\n",
    "            return image, label\n",
    "        else:\n",
    "            image = image.numpy()\n",
    "            return torch.from_numpy(image).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "66fc84af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_csv = pd.read_csv(os.path.join(base_path, \"train\",\"train_data.csv\"))\n",
    "\n",
    "tr_csv['path'] = tr_csv['filen_name'].apply(\n",
    "    lambda x: os.path.join(train_path,x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "af6ac8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teach_csv = tr_csv.sample(replace=False,n=3000)\n",
    "teach_index = teach_csv.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4254588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_csv = tr_csv.drop(teach_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8f5f660b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filen_name</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train0001.png</td>\n",
       "      <td>8</td>\n",
       "      <td>./train\\train0001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train0003.png</td>\n",
       "      <td>8</td>\n",
       "      <td>./train\\train0003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train0004.png</td>\n",
       "      <td>8</td>\n",
       "      <td>./train\\train0004.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train0009.png</td>\n",
       "      <td>8</td>\n",
       "      <td>./train\\train0009.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train0012.png</td>\n",
       "      <td>8</td>\n",
       "      <td>./train\\train0012.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>train4991.png</td>\n",
       "      <td>6</td>\n",
       "      <td>./train\\train4991.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>train4993.png</td>\n",
       "      <td>6</td>\n",
       "      <td>./train\\train4993.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>train4996.png</td>\n",
       "      <td>6</td>\n",
       "      <td>./train\\train4996.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>train4997.png</td>\n",
       "      <td>6</td>\n",
       "      <td>./train\\train4997.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>train4999.png</td>\n",
       "      <td>6</td>\n",
       "      <td>./train\\train4999.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         filen_name  label                   path\n",
       "0     train0001.png      8  ./train\\train0001.png\n",
       "2     train0003.png      8  ./train\\train0003.png\n",
       "3     train0004.png      8  ./train\\train0004.png\n",
       "8     train0009.png      8  ./train\\train0009.png\n",
       "11    train0012.png      8  ./train\\train0012.png\n",
       "...             ...    ...                    ...\n",
       "4990  train4991.png      6  ./train\\train4991.png\n",
       "4992  train4993.png      6  ./train\\train4993.png\n",
       "4995  train4996.png      6  ./train\\train4996.png\n",
       "4996  train4997.png      6  ./train\\train4997.png\n",
       "4998  train4999.png      6  ./train\\train4999.png\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "09d25954",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "    tr_csv['path'].values, tr_csv[\"label\"].values, test_size=0.5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "71c8edf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (1000,), (1000,), (1000,))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, valid_x.shape, train_y.shape,valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ab2bb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "teach_x, teach_y = teach_csv['path'].values, teach_csv[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d3b326f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teach_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0de62e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = MnistDataset(train_x, train_y, get_train_transforms())\n",
    "valid_ds = MnistDataset(valid_x, valid_y, get_valid_transforms())\n",
    "teach_ds = MnistDataset(teach_x,transform=get_train_transforms())\n",
    "train_size = len(train_ds)\n",
    "val_size = len(valid_ds)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = BATCH_SIZE, shuffle=False)\n",
    "teach_dl = DataLoader(teach_ds, batch_size = ULBATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "83999f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rows = 2\n",
    "# columns = 5\n",
    "# plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "# for idx in range(10) : \n",
    "#          # image index \n",
    "#     ttitle = \"Image{}\".format(idx+1 ) # image title\n",
    "#     plt.subplot(rows, columns, idx+1) # subplot \n",
    "#     plt.title(ttitle)   # title \n",
    "#     # // plt.axis('off')\n",
    "#     plt.xticks([])  # x = None \n",
    "#     plt.yticks([])  # y = None\n",
    "#     plt.imshow(np.squeeze(transform(image=np.array(Image.open(train_x[idx]).convert(\"RGB\"),dtype='float32'))['image']).permute(1,2,0)) \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e16ead29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = 2\n",
    "# columns = 5\n",
    "# plt.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "# for idx in range(10) : \n",
    "#          # image index \n",
    "#     ttitle = \"Image{}\".format(idx+1 ) # image title\n",
    "#     plt.subplot(rows, columns, idx+1) # subplot \n",
    "#     plt.title(ttitle)   # title \n",
    "#     # // plt.axis('off')\n",
    "#     plt.xticks([])  # x = None \n",
    "#     plt.yticks([])  # y = None\n",
    "#     plt.imshow(np.squeeze(transform(image=np.array(Image.open(train_x[idx]),dtype='float32'))['image'])) \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f6bf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, 3) # 합성곱 연산 (입력 채널 수: 3, 출력 채널 수: 6, 필터 크기: 5x5, stride=1(default))\n",
    "#         self.pool1 = nn.MaxPool2d(2,2) # 합성곱 연산 (필터크기 2x2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(16, 16, 3) # 합성곱 연산 (입력 채널 수: 6, 출력 채널수: 16, 필터 크기: 5x5, stride=1(default))\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2) # 합성곱 연산 (필터크기 2x2, stride=2)\n",
    "#         self.fc1 = nn.Linear(400,100) # 5x5 피쳐맵 16개를 일렬로 피면 16*5*5개의 노드가 생성됨.\n",
    "#         self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool1(F.relu(self.conv1(x))) # conv1 -> ReLU -> pool\n",
    "#         #print(x.shape)\n",
    "#         x = self.pool2(F.relu(self.conv2(x))) # conv2 -> ReLU -> pool2\n",
    "#         #print(x.shape)\n",
    "#         x = x.view(-1, 16*5*5) # 5x5 피쳐맵 16개를 일렬로 만든다.\n",
    "#         #print(x.shape)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         #print(x.shape)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return x\n",
    "# net = Net().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "48865b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inputs = nn.Conv2d(1,3,kernel_size=(1,1)) # 흑백 -> 3차원\n",
    "        self.model = torchvision.models.resnet34(pretrained = False) # 사전학습 x 구조만 load\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(1000,10) # 0~9 \n",
    "    def forward(self, x, softmax=True):\n",
    "        x = self.inputs(x)\n",
    "        x = self.model(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        if softmax:\n",
    "            x = F.softmax(x,dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "44924920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "net = Net().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss() # LogSoftmax + NLLLoss 다중분류\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "net.to(DEVICE)\n",
    "param = list(net.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                                    lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                                    last_epoch=-1,\n",
    "                                                    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b10a67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    # 참고: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "    \n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='./weight', k_num=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.k_num = k_num\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "#             print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f282ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader,criterion):\n",
    "    model.eval()\n",
    "    correct = 0 \n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.cuda()\n",
    "            output = model(data)\n",
    "            predicted = torch.max(output,1)[1]\n",
    "            correct += (predicted == labels.cuda()).sum()\n",
    "            loss += criterion(output, labels.cuda()).item()\n",
    "\n",
    "    return (float(correct)/len(valid_ds)) *100, (loss/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "96f03c4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-141-362a27537fd9>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(EPOCHS)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79613c956c5c43e39f8094d67c3953da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 : Train Loss : 0.00737 | Test Acc : 14.80000 | Test Loss : 2.297 \n",
      "Validation loss decreased (inf --> 2.296604).  Saving model ...\n",
      "Epoch: 10 : Train Loss : 0.00656 | Test Acc : 42.00000 | Test Loss : 2.046 \n",
      "Validation loss decreased (2.296604 --> 2.045809).  Saving model ...\n",
      "Epoch: 20 : Train Loss : 0.00624 | Test Acc : 52.50000 | Test Loss : 1.947 \n",
      "Validation loss decreased (2.045809 --> 1.946899).  Saving model ...\n",
      "Epoch: 30 : Train Loss : 0.00607 | Test Acc : 62.40000 | Test Loss : 1.886 \n",
      "Validation loss decreased (1.946899 --> 1.885909).  Saving model ...\n",
      "Epoch: 40 : Train Loss : 0.00595 | Test Acc : 68.60000 | Test Loss : 1.833 \n",
      "Validation loss decreased (1.885909 --> 1.833176).  Saving model ...\n",
      "Epoch: 50 : Train Loss : 0.00587 | Test Acc : 71.70000 | Test Loss : 1.811 \n",
      "Validation loss decreased (1.833176 --> 1.811416).  Saving model ...\n",
      "Epoch: 60 : Train Loss : 0.00581 | Test Acc : 73.50000 | Test Loss : 1.795 \n",
      "Validation loss decreased (1.811416 --> 1.794703).  Saving model ...\n",
      "Epoch: 70 : Train Loss : 0.00580 | Test Acc : 75.00000 | Test Loss : 1.783 \n",
      "Validation loss decreased (1.794703 --> 1.782607).  Saving model ...\n",
      "Epoch: 80 : Train Loss : 0.00573 | Test Acc : 76.40000 | Test Loss : 1.782 \n",
      "Validation loss decreased (1.782607 --> 1.781513).  Saving model ...\n",
      "Epoch: 90 : Train Loss : 0.00580 | Test Acc : 76.70000 | Test Loss : 1.781 \n",
      "Validation loss decreased (1.781513 --> 1.780714).  Saving model ...\n",
      "Epoch: 100 : Train Loss : 0.00577 | Test Acc : 75.60000 | Test Loss : 1.785 \n",
      "Epoch: 110 : Train Loss : 0.00578 | Test Acc : 76.70000 | Test Loss : 1.775 \n",
      "Validation loss decreased (1.780714 --> 1.775103).  Saving model ...\n",
      "Epoch: 120 : Train Loss : 0.00578 | Test Acc : 75.80000 | Test Loss : 1.780 \n",
      "Epoch: 130 : Train Loss : 0.00578 | Test Acc : 77.30000 | Test Loss : 1.773 \n",
      "Validation loss decreased (1.775103 --> 1.773002).  Saving model ...\n",
      "Epoch: 140 : Train Loss : 0.00574 | Test Acc : 77.40000 | Test Loss : 1.778 \n",
      "Epoch: 150 : Train Loss : 0.00574 | Test Acc : 75.70000 | Test Loss : 1.780 \n",
      "Epoch: 160 : Train Loss : 0.00574 | Test Acc : 77.60000 | Test Loss : 1.769 \n",
      "Validation loss decreased (1.773002 --> 1.769242).  Saving model ...\n",
      "Epoch: 170 : Train Loss : 0.00580 | Test Acc : 76.20000 | Test Loss : 1.781 \n",
      "Epoch: 180 : Train Loss : 0.00582 | Test Acc : 76.10000 | Test Loss : 1.775 \n",
      "Epoch: 190 : Train Loss : 0.00570 | Test Acc : 77.60000 | Test Loss : 1.778 \n",
      "Epoch: 200 : Train Loss : 0.00577 | Test Acc : 76.20000 | Test Loss : 1.778 \n",
      "Epoch: 210 : Train Loss : 0.00580 | Test Acc : 77.90000 | Test Loss : 1.768 \n",
      "Validation loss decreased (1.769242 --> 1.768238).  Saving model ...\n",
      "Epoch: 220 : Train Loss : 0.00569 | Test Acc : 77.80000 | Test Loss : 1.777 \n",
      "Epoch: 230 : Train Loss : 0.00579 | Test Acc : 77.00000 | Test Loss : 1.775 \n",
      "Epoch: 240 : Train Loss : 0.00573 | Test Acc : 76.20000 | Test Loss : 1.778 \n",
      "Epoch: 250 : Train Loss : 0.00581 | Test Acc : 76.60000 | Test Loss : 1.776 \n",
      "Epoch: 260 : Train Loss : 0.00575 | Test Acc : 77.60000 | Test Loss : 1.770 \n",
      "Epoch: 270 : Train Loss : 0.00578 | Test Acc : 76.30000 | Test Loss : 1.784 \n",
      "Epoch: 280 : Train Loss : 0.00581 | Test Acc : 75.40000 | Test Loss : 1.780 \n",
      "Epoch: 290 : Train Loss : 0.00579 | Test Acc : 76.50000 | Test Loss : 1.775 \n",
      "Epoch: 300 : Train Loss : 0.00579 | Test Acc : 76.90000 | Test Loss : 1.774 \n",
      "Epoch: 310 : Train Loss : 0.00571 | Test Acc : 77.50000 | Test Loss : 1.779 \n",
      "Epoch: 320 : Train Loss : 0.00577 | Test Acc : 76.80000 | Test Loss : 1.776 \n",
      "Epoch: 330 : Train Loss : 0.00578 | Test Acc : 76.50000 | Test Loss : 1.776 \n",
      "Epoch: 340 : Train Loss : 0.00584 | Test Acc : 75.80000 | Test Loss : 1.783 \n",
      "Epoch: 350 : Train Loss : 0.00577 | Test Acc : 76.10000 | Test Loss : 1.779 \n",
      "Epoch: 360 : Train Loss : 0.00572 | Test Acc : 77.70000 | Test Loss : 1.773 \n",
      "Epoch: 370 : Train Loss : 0.00577 | Test Acc : 76.00000 | Test Loss : 1.777 \n",
      "Epoch: 380 : Train Loss : 0.00575 | Test Acc : 75.40000 | Test Loss : 1.782 \n",
      "Epoch: 390 : Train Loss : 0.00574 | Test Acc : 77.10000 | Test Loss : 1.777 \n",
      "Epoch: 400 : Train Loss : 0.00576 | Test Acc : 78.20000 | Test Loss : 1.775 \n",
      "Epoch: 410 : Train Loss : 0.00579 | Test Acc : 78.00000 | Test Loss : 1.770 \n",
      "Epoch: 420 : Train Loss : 0.00580 | Test Acc : 77.20000 | Test Loss : 1.776 \n",
      "Epoch: 430 : Train Loss : 0.00579 | Test Acc : 77.40000 | Test Loss : 1.774 \n",
      "Epoch: 440 : Train Loss : 0.00580 | Test Acc : 77.00000 | Test Loss : 1.778 \n",
      "Epoch: 450 : Train Loss : 0.00573 | Test Acc : 75.80000 | Test Loss : 1.785 \n",
      "Epoch: 460 : Train Loss : 0.00576 | Test Acc : 77.10000 | Test Loss : 1.772 \n",
      "Epoch: 470 : Train Loss : 0.00574 | Test Acc : 76.80000 | Test Loss : 1.773 \n",
      "Epoch: 480 : Train Loss : 0.00579 | Test Acc : 76.10000 | Test Loss : 1.780 \n",
      "Epoch: 490 : Train Loss : 0.00583 | Test Acc : 75.70000 | Test Loss : 1.782 \n",
      "Epoch: 500 : Train Loss : 0.00578 | Test Acc : 76.20000 | Test Loss : 1.777 \n",
      "Epoch: 510 : Train Loss : 0.00581 | Test Acc : 76.70000 | Test Loss : 1.773 \n",
      "Epoch: 520 : Train Loss : 0.00573 | Test Acc : 77.10000 | Test Loss : 1.776 \n",
      "Epoch: 530 : Train Loss : 0.00576 | Test Acc : 76.70000 | Test Loss : 1.780 \n",
      "Epoch: 540 : Train Loss : 0.00577 | Test Acc : 76.50000 | Test Loss : 1.779 \n",
      "Epoch: 550 : Train Loss : 0.00577 | Test Acc : 77.30000 | Test Loss : 1.775 \n",
      "Epoch: 560 : Train Loss : 0.00572 | Test Acc : 77.80000 | Test Loss : 1.777 \n",
      "Epoch: 570 : Train Loss : 0.00582 | Test Acc : 78.00000 | Test Loss : 1.771 \n",
      "Epoch: 580 : Train Loss : 0.00580 | Test Acc : 77.30000 | Test Loss : 1.777 \n",
      "Epoch: 590 : Train Loss : 0.00576 | Test Acc : 77.80000 | Test Loss : 1.769 \n",
      "Epoch: 600 : Train Loss : 0.00578 | Test Acc : 77.90000 | Test Loss : 1.775 \n",
      "Epoch: 610 : Train Loss : 0.00574 | Test Acc : 78.20000 | Test Loss : 1.774 \n",
      "Epoch: 620 : Train Loss : 0.00575 | Test Acc : 77.30000 | Test Loss : 1.774 \n",
      "Epoch: 630 : Train Loss : 0.00573 | Test Acc : 75.20000 | Test Loss : 1.783 \n",
      "Epoch: 640 : Train Loss : 0.00576 | Test Acc : 76.80000 | Test Loss : 1.777 \n",
      "Epoch: 650 : Train Loss : 0.00579 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 660 : Train Loss : 0.00576 | Test Acc : 75.70000 | Test Loss : 1.780 \n",
      "Epoch: 670 : Train Loss : 0.00579 | Test Acc : 75.90000 | Test Loss : 1.777 \n",
      "Epoch: 680 : Train Loss : 0.00578 | Test Acc : 76.50000 | Test Loss : 1.776 \n",
      "Epoch: 690 : Train Loss : 0.00576 | Test Acc : 76.70000 | Test Loss : 1.780 \n",
      "Epoch: 700 : Train Loss : 0.00576 | Test Acc : 76.80000 | Test Loss : 1.774 \n",
      "Epoch: 710 : Train Loss : 0.00580 | Test Acc : 75.80000 | Test Loss : 1.775 \n",
      "Epoch: 720 : Train Loss : 0.00572 | Test Acc : 77.70000 | Test Loss : 1.770 \n",
      "Epoch: 730 : Train Loss : 0.00575 | Test Acc : 76.60000 | Test Loss : 1.776 \n",
      "Epoch: 740 : Train Loss : 0.00578 | Test Acc : 77.60000 | Test Loss : 1.775 \n",
      "Epoch: 750 : Train Loss : 0.00576 | Test Acc : 75.90000 | Test Loss : 1.781 \n",
      "Epoch: 760 : Train Loss : 0.00579 | Test Acc : 77.40000 | Test Loss : 1.773 \n",
      "Epoch: 770 : Train Loss : 0.00578 | Test Acc : 77.20000 | Test Loss : 1.776 \n",
      "Epoch: 780 : Train Loss : 0.00580 | Test Acc : 76.70000 | Test Loss : 1.774 \n",
      "Epoch: 790 : Train Loss : 0.00580 | Test Acc : 77.00000 | Test Loss : 1.773 \n",
      "Epoch: 800 : Train Loss : 0.00578 | Test Acc : 76.70000 | Test Loss : 1.773 \n",
      "Epoch: 810 : Train Loss : 0.00578 | Test Acc : 77.20000 | Test Loss : 1.773 \n",
      "Epoch: 820 : Train Loss : 0.00578 | Test Acc : 77.30000 | Test Loss : 1.776 \n",
      "Epoch: 830 : Train Loss : 0.00576 | Test Acc : 77.70000 | Test Loss : 1.770 \n",
      "Epoch: 840 : Train Loss : 0.00579 | Test Acc : 76.60000 | Test Loss : 1.777 \n",
      "Epoch: 850 : Train Loss : 0.00579 | Test Acc : 78.70000 | Test Loss : 1.767 \n",
      "Validation loss decreased (1.768238 --> 1.766883).  Saving model ...\n",
      "Epoch: 860 : Train Loss : 0.00577 | Test Acc : 76.00000 | Test Loss : 1.777 \n",
      "Epoch: 870 : Train Loss : 0.00575 | Test Acc : 76.70000 | Test Loss : 1.774 \n",
      "Epoch: 880 : Train Loss : 0.00575 | Test Acc : 76.20000 | Test Loss : 1.776 \n",
      "Epoch: 890 : Train Loss : 0.00579 | Test Acc : 76.30000 | Test Loss : 1.777 \n",
      "Epoch: 900 : Train Loss : 0.00573 | Test Acc : 77.30000 | Test Loss : 1.775 \n",
      "Epoch: 910 : Train Loss : 0.00583 | Test Acc : 77.70000 | Test Loss : 1.774 \n",
      "Epoch: 920 : Train Loss : 0.00579 | Test Acc : 76.70000 | Test Loss : 1.773 \n",
      "Epoch: 930 : Train Loss : 0.00579 | Test Acc : 76.70000 | Test Loss : 1.775 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 940 : Train Loss : 0.00576 | Test Acc : 76.50000 | Test Loss : 1.776 \n",
      "Epoch: 950 : Train Loss : 0.00584 | Test Acc : 76.40000 | Test Loss : 1.773 \n",
      "Epoch: 960 : Train Loss : 0.00574 | Test Acc : 75.90000 | Test Loss : 1.779 \n",
      "Epoch: 970 : Train Loss : 0.00575 | Test Acc : 76.30000 | Test Loss : 1.776 \n",
      "Epoch: 980 : Train Loss : 0.00581 | Test Acc : 77.50000 | Test Loss : 1.769 \n",
      "Epoch: 990 : Train Loss : 0.00580 | Test Acc : 77.10000 | Test Loss : 1.776 \n",
      "Epoch: 1000 : Train Loss : 0.00579 | Test Acc : 77.50000 | Test Loss : 1.771 \n",
      "Epoch: 1010 : Train Loss : 0.00584 | Test Acc : 76.90000 | Test Loss : 1.776 \n",
      "Epoch: 1020 : Train Loss : 0.00578 | Test Acc : 77.30000 | Test Loss : 1.768 \n",
      "Epoch: 1030 : Train Loss : 0.00581 | Test Acc : 75.40000 | Test Loss : 1.775 \n",
      "Epoch: 1040 : Train Loss : 0.00581 | Test Acc : 76.00000 | Test Loss : 1.778 \n",
      "Epoch: 1050 : Train Loss : 0.00582 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1060 : Train Loss : 0.00575 | Test Acc : 75.70000 | Test Loss : 1.782 \n",
      "Epoch: 1070 : Train Loss : 0.00579 | Test Acc : 76.80000 | Test Loss : 1.775 \n",
      "Epoch: 1080 : Train Loss : 0.00578 | Test Acc : 77.90000 | Test Loss : 1.776 \n",
      "Epoch: 1090 : Train Loss : 0.00580 | Test Acc : 77.00000 | Test Loss : 1.778 \n",
      "Epoch: 1100 : Train Loss : 0.00572 | Test Acc : 77.40000 | Test Loss : 1.772 \n",
      "Epoch: 1110 : Train Loss : 0.00581 | Test Acc : 76.20000 | Test Loss : 1.776 \n",
      "Epoch: 1120 : Train Loss : 0.00583 | Test Acc : 76.10000 | Test Loss : 1.778 \n",
      "Epoch: 1130 : Train Loss : 0.00575 | Test Acc : 76.80000 | Test Loss : 1.776 \n",
      "Epoch: 1140 : Train Loss : 0.00572 | Test Acc : 76.00000 | Test Loss : 1.784 \n",
      "Epoch: 1150 : Train Loss : 0.00580 | Test Acc : 77.30000 | Test Loss : 1.774 \n",
      "Epoch: 1160 : Train Loss : 0.00581 | Test Acc : 76.60000 | Test Loss : 1.775 \n",
      "Epoch: 1170 : Train Loss : 0.00576 | Test Acc : 77.40000 | Test Loss : 1.772 \n",
      "Epoch: 1180 : Train Loss : 0.00580 | Test Acc : 76.50000 | Test Loss : 1.781 \n",
      "Epoch: 1190 : Train Loss : 0.00580 | Test Acc : 75.50000 | Test Loss : 1.783 \n",
      "Epoch: 1200 : Train Loss : 0.00574 | Test Acc : 76.00000 | Test Loss : 1.781 \n",
      "Epoch: 1210 : Train Loss : 0.00584 | Test Acc : 76.60000 | Test Loss : 1.773 \n",
      "Epoch: 1220 : Train Loss : 0.00579 | Test Acc : 75.50000 | Test Loss : 1.785 \n",
      "Epoch: 1230 : Train Loss : 0.00578 | Test Acc : 76.60000 | Test Loss : 1.775 \n",
      "Epoch: 1240 : Train Loss : 0.00570 | Test Acc : 76.60000 | Test Loss : 1.778 \n",
      "Epoch: 1250 : Train Loss : 0.00581 | Test Acc : 77.40000 | Test Loss : 1.774 \n",
      "Epoch: 1260 : Train Loss : 0.00571 | Test Acc : 76.10000 | Test Loss : 1.782 \n",
      "Epoch: 1270 : Train Loss : 0.00575 | Test Acc : 75.50000 | Test Loss : 1.781 \n",
      "Epoch: 1280 : Train Loss : 0.00579 | Test Acc : 76.20000 | Test Loss : 1.777 \n",
      "Epoch: 1290 : Train Loss : 0.00581 | Test Acc : 76.20000 | Test Loss : 1.774 \n",
      "Epoch: 1300 : Train Loss : 0.00577 | Test Acc : 76.20000 | Test Loss : 1.783 \n",
      "Epoch: 1310 : Train Loss : 0.00581 | Test Acc : 78.10000 | Test Loss : 1.767 \n",
      "Epoch: 1320 : Train Loss : 0.00581 | Test Acc : 76.60000 | Test Loss : 1.774 \n",
      "Epoch: 1330 : Train Loss : 0.00578 | Test Acc : 76.20000 | Test Loss : 1.782 \n",
      "Epoch: 1340 : Train Loss : 0.00578 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 1350 : Train Loss : 0.00576 | Test Acc : 76.30000 | Test Loss : 1.779 \n",
      "Epoch: 1360 : Train Loss : 0.00579 | Test Acc : 76.00000 | Test Loss : 1.775 \n",
      "Epoch: 1370 : Train Loss : 0.00576 | Test Acc : 76.10000 | Test Loss : 1.782 \n",
      "Epoch: 1380 : Train Loss : 0.00573 | Test Acc : 77.60000 | Test Loss : 1.769 \n",
      "Epoch: 1390 : Train Loss : 0.00579 | Test Acc : 77.10000 | Test Loss : 1.773 \n",
      "Epoch: 1400 : Train Loss : 0.00576 | Test Acc : 76.90000 | Test Loss : 1.774 \n",
      "Epoch: 1410 : Train Loss : 0.00581 | Test Acc : 76.80000 | Test Loss : 1.775 \n",
      "Epoch: 1420 : Train Loss : 0.00581 | Test Acc : 76.50000 | Test Loss : 1.776 \n",
      "Epoch: 1430 : Train Loss : 0.00582 | Test Acc : 76.40000 | Test Loss : 1.773 \n",
      "Epoch: 1440 : Train Loss : 0.00578 | Test Acc : 76.60000 | Test Loss : 1.776 \n",
      "Epoch: 1450 : Train Loss : 0.00574 | Test Acc : 77.70000 | Test Loss : 1.771 \n",
      "Epoch: 1460 : Train Loss : 0.00583 | Test Acc : 76.80000 | Test Loss : 1.774 \n",
      "Epoch: 1470 : Train Loss : 0.00578 | Test Acc : 76.70000 | Test Loss : 1.775 \n",
      "Epoch: 1480 : Train Loss : 0.00577 | Test Acc : 78.10000 | Test Loss : 1.772 \n",
      "Epoch: 1490 : Train Loss : 0.00574 | Test Acc : 77.70000 | Test Loss : 1.771 \n",
      "Epoch: 1500 : Train Loss : 0.00584 | Test Acc : 77.50000 | Test Loss : 1.774 \n",
      "Epoch: 1510 : Train Loss : 0.00578 | Test Acc : 76.20000 | Test Loss : 1.775 \n",
      "Epoch: 1520 : Train Loss : 0.00573 | Test Acc : 76.90000 | Test Loss : 1.773 \n",
      "Epoch: 1530 : Train Loss : 0.00578 | Test Acc : 76.20000 | Test Loss : 1.779 \n",
      "Epoch: 1540 : Train Loss : 0.00575 | Test Acc : 77.70000 | Test Loss : 1.776 \n",
      "Epoch: 1550 : Train Loss : 0.00582 | Test Acc : 76.80000 | Test Loss : 1.776 \n",
      "Epoch: 1560 : Train Loss : 0.00576 | Test Acc : 75.90000 | Test Loss : 1.780 \n",
      "Epoch: 1570 : Train Loss : 0.00573 | Test Acc : 76.50000 | Test Loss : 1.783 \n",
      "Epoch: 1580 : Train Loss : 0.00575 | Test Acc : 77.80000 | Test Loss : 1.768 \n",
      "Epoch: 1590 : Train Loss : 0.00578 | Test Acc : 76.30000 | Test Loss : 1.776 \n",
      "Epoch: 1600 : Train Loss : 0.00575 | Test Acc : 77.10000 | Test Loss : 1.774 \n",
      "Epoch: 1610 : Train Loss : 0.00579 | Test Acc : 76.80000 | Test Loss : 1.777 \n",
      "Epoch: 1620 : Train Loss : 0.00582 | Test Acc : 76.10000 | Test Loss : 1.780 \n",
      "Epoch: 1630 : Train Loss : 0.00573 | Test Acc : 76.00000 | Test Loss : 1.782 \n",
      "Epoch: 1640 : Train Loss : 0.00579 | Test Acc : 77.40000 | Test Loss : 1.773 \n",
      "Epoch: 1650 : Train Loss : 0.00578 | Test Acc : 76.50000 | Test Loss : 1.779 \n",
      "Epoch: 1660 : Train Loss : 0.00578 | Test Acc : 76.20000 | Test Loss : 1.775 \n",
      "Epoch: 1670 : Train Loss : 0.00577 | Test Acc : 76.30000 | Test Loss : 1.780 \n",
      "Epoch: 1680 : Train Loss : 0.00576 | Test Acc : 75.70000 | Test Loss : 1.783 \n",
      "Epoch: 1690 : Train Loss : 0.00576 | Test Acc : 76.40000 | Test Loss : 1.778 \n",
      "Epoch: 1700 : Train Loss : 0.00580 | Test Acc : 76.60000 | Test Loss : 1.776 \n",
      "Epoch: 1710 : Train Loss : 0.00577 | Test Acc : 76.40000 | Test Loss : 1.777 \n",
      "Epoch: 1720 : Train Loss : 0.00579 | Test Acc : 77.10000 | Test Loss : 1.772 \n",
      "Epoch: 1730 : Train Loss : 0.00582 | Test Acc : 74.80000 | Test Loss : 1.785 \n",
      "Epoch: 1740 : Train Loss : 0.00580 | Test Acc : 76.60000 | Test Loss : 1.779 \n",
      "Epoch: 1750 : Train Loss : 0.00579 | Test Acc : 77.20000 | Test Loss : 1.775 \n",
      "Epoch: 1760 : Train Loss : 0.00571 | Test Acc : 75.50000 | Test Loss : 1.786 \n",
      "Epoch: 1770 : Train Loss : 0.00580 | Test Acc : 78.30000 | Test Loss : 1.773 \n",
      "Epoch: 1780 : Train Loss : 0.00576 | Test Acc : 76.90000 | Test Loss : 1.777 \n",
      "Epoch: 1790 : Train Loss : 0.00580 | Test Acc : 77.60000 | Test Loss : 1.771 \n",
      "Epoch: 1800 : Train Loss : 0.00576 | Test Acc : 77.90000 | Test Loss : 1.768 \n",
      "Epoch: 1810 : Train Loss : 0.00572 | Test Acc : 76.80000 | Test Loss : 1.781 \n",
      "Epoch: 1820 : Train Loss : 0.00575 | Test Acc : 77.90000 | Test Loss : 1.768 \n",
      "Epoch: 1830 : Train Loss : 0.00582 | Test Acc : 75.90000 | Test Loss : 1.779 \n",
      "Epoch: 1840 : Train Loss : 0.00577 | Test Acc : 77.90000 | Test Loss : 1.770 \n",
      "Epoch: 1850 : Train Loss : 0.00581 | Test Acc : 78.20000 | Test Loss : 1.775 \n",
      "Early stopping!!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "# normal train\n",
    "def train_supervised(model,optimizer,crtierion, train_loader, test_loader):\n",
    "    EPOCHS = 3000\n",
    "    early_stopping = EarlyStopping(patience = 100, verbose = True)\n",
    "    model.train()\n",
    "    for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "        correct = 0\n",
    "        running_loss = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            output = model(X_batch)\n",
    "            labeled_loss = criterion(output, y_batch)\n",
    "                       \n",
    "            optimizer.zero_grad()\n",
    "            labeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += labeled_loss.item()\n",
    "        scheduler.step()\n",
    "        if epoch %10 == 0:\n",
    "            test_acc, test_loss = evaluate(model, test_loader,criterion)\n",
    "            print('Epoch: {} : Train Loss : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, running_loss/(10 * len(train_ds)), test_acc, test_loss))\n",
    "            model.train()\n",
    "            early_stopping(float(test_loss), model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!!\")\n",
    "            break            \n",
    "            \n",
    "train_supervised(net,optimizer,criterion, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4091f3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc : 78.20000 | Test Loss : 1.775 \n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = evaluate(net, valid_dl,criterion)\n",
    "print('Test Acc : {:.5f} | Test Loss : {:.3f} '.format(test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "54e4e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 100 # HyperParameter \n",
    "T2 = 700\n",
    "af = 3\n",
    "\n",
    "def alpha_weight(epoch):\n",
    "    if epoch < T1:\n",
    "        return 0.0\n",
    "    elif epoch > T2:\n",
    "        return af\n",
    "    else:\n",
    "         return ((epoch-T1) / (T2-T1))*af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5d254043",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-144-a4968f53b0c2>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-144-a4968f53b0c2>\"\u001b[1;36m, line \u001b[1;32m27\u001b[0m\n\u001b[1;33m    \"\"\" ONLY FOR VISUALIZATION\"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#Psuedo Labeling \n",
    "acc_scores = []\n",
    "unlabel = []\n",
    "pseudo_label = []\n",
    "\n",
    "alpha_log = []\n",
    "test_acc_log = []\n",
    "test_loss_log = []\n",
    "def semisup_train(model,optimizer,criterion, train_loader, unlabeled_loader, test_loader):\n",
    "    EPOCHS = 2000\n",
    "    early_stopping = EarlyStopping(patience = 100, verbose = True)\n",
    "    # 기본 Epoch를 사용하는 대신 a_weight를 계산하기 위해 step 변수 사용\n",
    "    # 따라서 모델이 더 빠르게 수렴함\n",
    "    step = 1000 \n",
    "    \n",
    "    model.train()\n",
    "    for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "        for batch_idx, x_unlabeled in enumerate(unlabeled_loader):\n",
    "            \n",
    "            \n",
    "            # pseudo label을 가져오기 위해 forward pass\n",
    "            x_unlabeled = x_unlabeled.cuda()\n",
    "            model.eval()\n",
    "            output_unlabeled = model(x_unlabeled)\n",
    "            _, pseudo_labeled = torch.max(output_unlabeled, 1)\n",
    "            model.train()\n",
    "             \"\"\" ONLY FOR VISUALIZATION\"\"\"\n",
    "            if (batch_idx < 3) and (epoch % 10 == 0):\n",
    "                unlabel.append(x_unlabeled.cpu())\n",
    "                pseudo_label.append(pseudo_labeled.cpu())\n",
    "                 \"\"\" ********************** \"\"\"\n",
    "            \n",
    "            # pseudo label 을 사용하여 레이블이 지정되지 않은 손실을 계산\n",
    "            output = model(x_unlabeled)\n",
    "            unlabeled_loss = alpha_weight(step) * criterion(output, pseudo_labeled)   \n",
    "            \n",
    "            # 역전파\n",
    "            optimizer.zero_grad()\n",
    "            unlabeled_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # 50개 배치마다 레이블이 지정된 데이터에 대해 한단계씩 학습함\n",
    "            if batch_idx % 50 == 0:\n",
    "                \n",
    "                # Normal training procedure\n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                    X_batch = X_batch.cuda()\n",
    "                    y_batch = y_batch.cuda()\n",
    "                    output = model(X_batch)\n",
    "                    labeled_loss = criterion(output, y_batch)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    labeled_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Now we increment step by 1\n",
    "                step += 1\n",
    "                \n",
    "\n",
    "        test_acc, test_loss =evaluate(model, test_loader,criterion)\n",
    "        early_stopping(float(test_loss), model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!!\")\n",
    "            break            \n",
    "            print('Epoch: {} : Train Loss : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, running_loss/(10 * len(train_ds)), test_acc, test_loss))\n",
    "            model.train()\n",
    "        print('Epoch: {} : Alpha Weight : {:.5f} | Test Acc : {:.5f} | Test Loss : {:.3f} '.format(epoch, alpha_weight(step), test_acc, test_loss))\n",
    "        \n",
    "        \"\"\" LOGGING VALUES \"\"\"\n",
    "        alpha_log.append(alpha_weight(step))\n",
    "        test_acc_log.append(test_acc/100)\n",
    "        test_loss_log.append(test_loss)\n",
    "        \"\"\" ************** \"\"\"\n",
    "        model.train()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a6d84c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-70-933b17ad1071>:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm_notebook(range(EPOCHS)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794d18ba7e7d469894a37eee920c70b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 2 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 3 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 4 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 5 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 6 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 7 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 8 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 9 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 10 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 11 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 12 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 13 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 14 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 15 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 16 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 17 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 18 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 19 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 20 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 21 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 22 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 23 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.768 \n",
      "Epoch: 24 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.771 \n",
      "Epoch: 25 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 26 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 27 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 28 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 29 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 30 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 31 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 32 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 33 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 34 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 35 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 36 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 37 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.770 \n",
      "Epoch: 38 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 39 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 40 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 41 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 42 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 43 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 44 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 45 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 46 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 47 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 48 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 49 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 50 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 51 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 52 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 53 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 54 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 55 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 56 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 57 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 58 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.767 \n",
      "Epoch: 59 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 60 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 61 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 62 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 63 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 64 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 65 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 66 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 67 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 68 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.769 \n",
      "Epoch: 69 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 70 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 71 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 72 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 73 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 74 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 75 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 76 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 77 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 78 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 79 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 80 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 81 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 82 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 83 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.768 \n",
      "Epoch: 84 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 85 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.772 \n",
      "Epoch: 86 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 87 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 88 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 89 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 90 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 91 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 92 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 93 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 94 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 95 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 96 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 97 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 98 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 99 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 100 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 101 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 102 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 103 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 104 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 105 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 107 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 108 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 109 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 110 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 111 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 112 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 113 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.773 \n",
      "Epoch: 114 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 115 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 116 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 117 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 118 : Alpha Weight : 3.00000 | Test Acc : 77.20000 | Test Loss : 1.771 \n",
      "Epoch: 119 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 120 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 121 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 122 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 123 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 124 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 125 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 126 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 127 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 128 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 129 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 130 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 131 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 132 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 133 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 134 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 135 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 136 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 137 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 138 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 139 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 140 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 141 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.769 \n",
      "Epoch: 142 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 143 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 144 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 145 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 146 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 147 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 148 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 149 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 150 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 151 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 152 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 153 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 154 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 155 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 156 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 157 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 158 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 159 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 160 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 161 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 162 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 163 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 164 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 165 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 166 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 167 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 168 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 169 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 170 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 171 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 172 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 173 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 174 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 175 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 176 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 177 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 178 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 179 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 180 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 181 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 182 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.773 \n",
      "Epoch: 183 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 184 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 185 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 186 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 187 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 188 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 189 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 190 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 191 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 192 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 193 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 194 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 195 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 196 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 197 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 198 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 199 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 200 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 201 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 202 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 203 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 204 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 205 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 206 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 207 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 208 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 209 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 210 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.773 \n",
      "Epoch: 211 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 212 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 213 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 214 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 215 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 216 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.772 \n",
      "Epoch: 217 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 218 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 219 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 220 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 221 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 222 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 223 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 224 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 225 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 226 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 227 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 228 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 229 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 230 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 231 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 232 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 233 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 234 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 235 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 236 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 237 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 238 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 239 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 240 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 241 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 242 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 243 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 244 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 245 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 246 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 247 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 248 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 249 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 250 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 251 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 252 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.773 \n",
      "Epoch: 253 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 254 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 255 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 256 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 257 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 258 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 259 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 260 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 261 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 262 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 263 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 264 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 265 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 266 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 267 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 268 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 269 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 270 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 271 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 272 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 273 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 274 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 275 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 276 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 277 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 278 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 279 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 280 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.772 \n",
      "Epoch: 281 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 282 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 283 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 284 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 285 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 286 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 287 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 288 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 289 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 290 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 291 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 292 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 293 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 294 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 295 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 296 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 297 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 298 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 299 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 300 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 301 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 302 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 303 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 304 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 305 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 306 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 307 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 308 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 309 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 310 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.770 \n",
      "Epoch: 311 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 312 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 313 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 315 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 316 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 317 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 318 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 319 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 320 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 321 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 322 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 323 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 324 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 325 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 326 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.768 \n",
      "Epoch: 327 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 328 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 329 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 330 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 331 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 332 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 333 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 334 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 335 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 336 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 337 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 338 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 339 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 340 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 341 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.769 \n",
      "Epoch: 342 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.767 \n",
      "Epoch: 343 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 344 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 345 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 346 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 347 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 348 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 349 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 350 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 351 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 352 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 353 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 354 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 355 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 356 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 357 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 358 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 359 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 360 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 361 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 362 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 363 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 364 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 365 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 366 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 367 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 368 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 369 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 370 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 371 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 372 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 373 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 374 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 375 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 376 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 377 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 378 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 379 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 380 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 381 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 382 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 383 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 384 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 385 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 386 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 387 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 388 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.773 \n",
      "Epoch: 389 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 390 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 391 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 392 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 393 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 394 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 395 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 396 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 397 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.768 \n",
      "Epoch: 398 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 399 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 400 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 401 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 402 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 403 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 404 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 405 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 406 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 407 : Alpha Weight : 3.00000 | Test Acc : 77.20000 | Test Loss : 1.770 \n",
      "Epoch: 408 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 409 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.772 \n",
      "Epoch: 410 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 411 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 412 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 413 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 414 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 415 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 416 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.768 \n",
      "Epoch: 417 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 418 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 419 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 420 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 421 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 422 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 423 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 424 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 425 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 426 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 427 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 428 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 429 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 430 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.772 \n",
      "Epoch: 431 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 432 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 433 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 434 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 435 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 436 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 437 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 438 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 439 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 440 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 441 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 442 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 443 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 444 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 445 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 446 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 447 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 448 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 449 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 450 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 451 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 452 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 453 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 454 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 455 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 456 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 457 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 458 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 459 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 460 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 461 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 462 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 463 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 464 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 465 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 466 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 467 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 468 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 469 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 470 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 471 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 472 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 473 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 474 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 475 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 476 : Alpha Weight : 3.00000 | Test Acc : 77.20000 | Test Loss : 1.769 \n",
      "Epoch: 477 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 478 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 479 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 480 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 481 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 482 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 483 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 484 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 485 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 486 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 487 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 488 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 489 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 490 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 491 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 492 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 493 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 494 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 495 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 496 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.767 \n",
      "Epoch: 497 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 498 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 499 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 500 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 501 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 502 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 503 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 504 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 505 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 506 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 507 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 508 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 509 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 510 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 511 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 512 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 513 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 514 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 515 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 516 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 517 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 518 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 519 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 520 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 521 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 522 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 523 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 524 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 525 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 526 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 527 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 528 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 529 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 530 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 531 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 532 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 533 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 534 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 535 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 536 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 537 : Alpha Weight : 3.00000 | Test Acc : 75.70000 | Test Loss : 1.771 \n",
      "Epoch: 538 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 539 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 540 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 541 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 542 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 543 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 544 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 545 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 546 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 547 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 548 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 549 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 550 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 551 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 552 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 553 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 554 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 555 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 556 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 557 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 558 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 559 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 560 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 561 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 562 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 563 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 564 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 565 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 566 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 567 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 568 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 569 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 570 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.768 \n",
      "Epoch: 571 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 572 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 573 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 574 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 575 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 576 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 577 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 578 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 579 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 580 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 581 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.771 \n",
      "Epoch: 582 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 583 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 584 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 585 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 586 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 587 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 588 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 589 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 590 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 591 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 592 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 593 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 594 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 595 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 596 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 597 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 598 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 599 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 600 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 601 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 602 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 603 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 604 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 605 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 606 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 607 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 608 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 609 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 610 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 611 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 612 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 613 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 614 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 615 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 616 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 617 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 618 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 619 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 620 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.772 \n",
      "Epoch: 621 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 622 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 623 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.768 \n",
      "Epoch: 624 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 625 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 626 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 627 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 628 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 629 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 630 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 631 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 632 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 633 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 634 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.773 \n",
      "Epoch: 635 : Alpha Weight : 3.00000 | Test Acc : 75.70000 | Test Loss : 1.770 \n",
      "Epoch: 636 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 637 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 638 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 639 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 640 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 641 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 642 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 643 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.770 \n",
      "Epoch: 644 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 645 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 646 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 647 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 648 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 649 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 650 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 651 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.772 \n",
      "Epoch: 652 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 653 : Alpha Weight : 3.00000 | Test Acc : 75.70000 | Test Loss : 1.772 \n",
      "Epoch: 654 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 655 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 656 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 657 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 658 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.774 \n",
      "Epoch: 659 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 660 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 661 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 662 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 663 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 664 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 665 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 666 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.772 \n",
      "Epoch: 667 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 668 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 669 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 670 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 671 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 672 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 673 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 674 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 675 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 676 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 677 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 678 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 679 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 680 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 681 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.773 \n",
      "Epoch: 682 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 683 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 684 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 685 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 686 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 687 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 688 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 689 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 690 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 691 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 692 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 693 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 694 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 695 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 696 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 697 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 698 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 699 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 700 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 701 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 702 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 703 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 704 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.769 \n",
      "Epoch: 705 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 706 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 707 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 708 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 709 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 710 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 711 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 712 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 713 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 714 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 715 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 716 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 717 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 718 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 719 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 720 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 721 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 722 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 723 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 724 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 725 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 726 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 727 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 728 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 729 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 730 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 731 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 732 : Alpha Weight : 3.00000 | Test Acc : 77.20000 | Test Loss : 1.770 \n",
      "Epoch: 733 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 734 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 735 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 736 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 737 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 738 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.772 \n",
      "Epoch: 739 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 740 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 741 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 742 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 743 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 744 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 745 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 746 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 747 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 748 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 749 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 750 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 751 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 752 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 753 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 754 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.769 \n",
      "Epoch: 755 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 756 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 757 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 758 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 759 : Alpha Weight : 3.00000 | Test Acc : 75.70000 | Test Loss : 1.769 \n",
      "Epoch: 760 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 761 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 762 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 763 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 764 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 765 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 766 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 767 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 768 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 769 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 770 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 771 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 772 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 773 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 774 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 775 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 776 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 777 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 778 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 779 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 780 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 781 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 782 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 783 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 784 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 785 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 786 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 787 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 788 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 789 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 790 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 791 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 792 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 793 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 794 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 795 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 796 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 797 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 798 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 799 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 800 : Alpha Weight : 3.00000 | Test Acc : 75.70000 | Test Loss : 1.771 \n",
      "Epoch: 801 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 802 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 803 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 804 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 805 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 806 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 807 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 808 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 809 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 810 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 811 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 812 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 813 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 814 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 815 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 816 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 817 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 818 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 819 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 820 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 821 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 822 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.773 \n",
      "Epoch: 823 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 824 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 825 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 826 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 827 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 828 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 829 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 830 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 831 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 832 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 833 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 834 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.773 \n",
      "Epoch: 835 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 836 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 837 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 838 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 839 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 840 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 841 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 842 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 843 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 844 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 845 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 846 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 847 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 848 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 849 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 850 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 851 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 852 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 853 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 854 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 855 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 856 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 857 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 858 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 859 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 860 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 861 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 862 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 863 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 864 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 865 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 866 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 867 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 868 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 869 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 870 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.772 \n",
      "Epoch: 871 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 872 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 873 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 874 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 875 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 876 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 877 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 878 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 879 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 880 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 881 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.772 \n",
      "Epoch: 882 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 883 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 884 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 885 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 886 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 887 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 888 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.769 \n",
      "Epoch: 889 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 890 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 891 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 892 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 893 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 894 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 895 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 896 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 897 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 898 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 899 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 900 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.771 \n",
      "Epoch: 901 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 902 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 903 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 904 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 905 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 906 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.772 \n",
      "Epoch: 907 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 908 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 909 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 910 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 911 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 912 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 913 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 914 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 915 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 916 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 917 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 918 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 919 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 920 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 921 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 922 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 923 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 924 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 925 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 926 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 927 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 928 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 929 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 930 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 931 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 932 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 933 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 934 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 935 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 936 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 937 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 938 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 939 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 940 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 941 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 942 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 943 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 944 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 945 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 946 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 947 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 948 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 949 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 950 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 951 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 952 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 953 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 954 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 955 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 956 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 957 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.767 \n",
      "Epoch: 958 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 959 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 960 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 961 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 962 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 963 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 964 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 965 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 966 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 967 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 968 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 969 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.773 \n",
      "Epoch: 970 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 971 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 972 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 973 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 974 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 975 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 976 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 977 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 978 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 979 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 980 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 981 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 982 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 983 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 984 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 985 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 986 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 987 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 988 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 989 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 990 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 991 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 992 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 993 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 994 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 995 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 996 : Alpha Weight : 3.00000 | Test Acc : 77.20000 | Test Loss : 1.769 \n",
      "Epoch: 997 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 998 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 999 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1000 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 1001 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1002 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1003 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1004 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1005 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1006 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.768 \n",
      "Epoch: 1007 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1008 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1009 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1010 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1011 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1012 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1013 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 1014 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1015 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1016 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1017 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1018 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 1019 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1020 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1021 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1022 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1023 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.771 \n",
      "Epoch: 1024 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1025 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1026 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1027 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1028 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1029 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1030 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1031 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1032 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1033 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1034 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1035 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 1036 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1037 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1038 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1039 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1040 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1041 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1042 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1043 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1044 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 1045 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1046 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1047 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1048 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1049 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1050 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1051 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1052 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1053 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 1054 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 1055 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1056 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1057 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1058 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1059 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1060 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1061 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1062 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1063 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1064 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1065 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1066 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1067 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1068 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1069 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1070 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 1071 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1072 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1073 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.768 \n",
      "Epoch: 1074 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1075 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.768 \n",
      "Epoch: 1076 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1077 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1078 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1079 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1080 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1081 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1082 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1083 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1084 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1085 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 1086 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1087 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1088 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1089 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1090 : Alpha Weight : 3.00000 | Test Acc : 77.20000 | Test Loss : 1.769 \n",
      "Epoch: 1091 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.768 \n",
      "Epoch: 1092 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1093 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1094 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1095 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1096 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1097 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1098 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1099 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1100 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1101 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1102 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1103 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1104 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1105 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1106 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1107 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 1108 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1109 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1110 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1111 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1112 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.771 \n",
      "Epoch: 1113 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1114 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1115 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1116 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1117 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1118 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1119 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.768 \n",
      "Epoch: 1120 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 1121 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1122 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1123 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1124 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1125 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1126 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1127 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1128 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 1129 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1130 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1131 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1132 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1133 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.771 \n",
      "Epoch: 1134 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1135 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1136 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1137 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1138 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1139 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1140 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1141 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 1142 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1143 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1144 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1145 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1146 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1147 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1148 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1149 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1150 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 1151 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1152 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1153 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1154 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.768 \n",
      "Epoch: 1155 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1156 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1157 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1158 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 1159 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1160 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1161 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 1162 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1163 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1164 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1165 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1166 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1167 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1168 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1169 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1170 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1171 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1172 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1173 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1174 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 1175 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1176 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1177 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1178 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1179 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1180 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1181 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1182 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1183 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 1184 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1185 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1186 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1187 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1188 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1189 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1190 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1191 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1192 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1193 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1194 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1195 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1196 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1197 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1198 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1199 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1200 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1201 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1202 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1203 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 1204 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1205 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1206 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 1207 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1208 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1209 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 1210 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1211 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1212 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1213 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.768 \n",
      "Epoch: 1214 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1215 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1216 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1217 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1218 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1219 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1220 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1221 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1222 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1223 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1224 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.769 \n",
      "Epoch: 1225 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1226 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.769 \n",
      "Epoch: 1227 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 1228 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1229 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1230 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 1231 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1232 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1233 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1234 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1235 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1236 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1237 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1238 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1239 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1240 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1241 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1242 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1243 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1244 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1245 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1246 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1247 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1248 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1249 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1250 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1251 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1252 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1253 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 1254 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.769 \n",
      "Epoch: 1255 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.769 \n",
      "Epoch: 1256 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1257 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1258 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 1259 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1260 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1261 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1262 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1263 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 1264 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1265 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1266 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1267 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1268 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1269 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1270 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1271 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1272 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1273 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1274 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1275 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1276 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1277 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1278 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1279 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1280 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1281 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1282 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1283 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1284 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1285 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1286 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1287 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1288 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1289 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1290 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1291 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1292 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1293 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1294 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.772 \n",
      "Epoch: 1295 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1296 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1297 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1298 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1299 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1300 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1301 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1302 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1303 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1304 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.767 \n",
      "Epoch: 1305 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1306 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1307 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 1308 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1309 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1310 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1311 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1312 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1313 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1314 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1315 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 1316 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1317 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1318 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1319 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1320 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1321 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1322 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1323 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1324 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1325 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1326 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1327 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 1328 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 1329 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1330 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1331 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1332 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1333 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 1334 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1335 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.769 \n",
      "Epoch: 1336 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 1337 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1338 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1339 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1340 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1341 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1342 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1343 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1344 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1345 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1346 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 1347 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.771 \n",
      "Epoch: 1348 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1349 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 1350 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1351 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.771 \n",
      "Epoch: 1352 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1353 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 1354 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1355 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1356 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1357 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1358 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1359 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1360 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1361 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1362 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1363 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1364 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1365 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1366 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1367 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1368 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1369 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1370 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1371 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1372 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1373 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1374 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1375 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1376 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1377 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1378 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1379 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1380 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1381 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 1382 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1383 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1384 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.771 \n",
      "Epoch: 1385 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1386 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1387 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1388 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1389 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1390 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1391 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1392 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 1393 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1394 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1395 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1396 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1397 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1398 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1399 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1400 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1401 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1402 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1403 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1404 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1405 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1406 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1407 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1408 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.772 \n",
      "Epoch: 1409 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1410 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1411 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1412 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 1413 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1414 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1415 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1416 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1417 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1418 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1419 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1420 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1421 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1422 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1423 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1424 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1425 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1426 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1427 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1428 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1429 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1430 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1431 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1432 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1433 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1434 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1435 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1436 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1437 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1438 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1439 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1440 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1441 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1442 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1443 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1444 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1445 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1446 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1447 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1448 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 1449 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1450 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.772 \n",
      "Epoch: 1451 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1452 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1453 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1454 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1455 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1456 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1457 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1458 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1459 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1460 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1461 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1462 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1463 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1464 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1465 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1466 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1467 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1468 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1469 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 1470 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1471 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.768 \n",
      "Epoch: 1472 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1473 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.773 \n",
      "Epoch: 1474 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1475 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1476 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1477 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1478 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1479 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1480 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 1481 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1482 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1483 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1484 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1485 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1486 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1487 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1488 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1489 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1490 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 1491 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1492 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 1493 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1494 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1495 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1496 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1497 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1498 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1499 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1500 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1501 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 1502 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1503 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1504 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1505 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1506 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1507 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1508 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1509 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1510 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1511 : Alpha Weight : 3.00000 | Test Acc : 77.30000 | Test Loss : 1.770 \n",
      "Epoch: 1512 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1513 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.767 \n",
      "Epoch: 1514 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1515 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1516 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 1517 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1518 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1519 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1520 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1521 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1522 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1523 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1524 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1525 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1526 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1527 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1528 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1529 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1530 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1531 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1532 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1533 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1534 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1535 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1536 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 1537 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1538 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1539 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1540 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1541 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1542 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1543 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1544 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1545 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1546 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1547 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1548 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 1549 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1550 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1551 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1552 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1553 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1554 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1555 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1556 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1557 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1558 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1559 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1560 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1561 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1562 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1563 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1564 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1565 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1566 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1567 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1568 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1569 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1570 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1571 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1572 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1573 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1574 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1575 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1576 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1577 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1578 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1579 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1580 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1581 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1582 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1583 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1584 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1585 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1586 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1587 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1588 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1589 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1590 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1591 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1592 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1593 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1594 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1595 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1596 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1597 : Alpha Weight : 3.00000 | Test Acc : 75.70000 | Test Loss : 1.771 \n",
      "Epoch: 1598 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 1599 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1600 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1601 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1602 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1603 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1604 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1605 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1606 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1607 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1608 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1609 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1610 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1611 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1612 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1613 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1614 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1615 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1616 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1617 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1618 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1619 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1620 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1621 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1622 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1623 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.772 \n",
      "Epoch: 1624 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1625 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1626 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1627 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1628 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1629 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1630 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1631 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1632 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1633 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1634 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.769 \n",
      "Epoch: 1635 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 1636 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1637 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 1638 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1639 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1640 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1641 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1642 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1643 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1644 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1645 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 1646 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1647 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1648 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1649 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1650 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.768 \n",
      "Epoch: 1651 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1652 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1653 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.771 \n",
      "Epoch: 1654 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1655 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1656 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1657 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1658 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1659 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1660 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1661 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1662 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1663 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1664 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.768 \n",
      "Epoch: 1665 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1666 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1667 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1668 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1669 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1670 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1671 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1672 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1673 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1674 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1675 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1676 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1677 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.769 \n",
      "Epoch: 1678 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1679 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1680 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1681 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1682 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1683 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1684 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1685 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1686 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1687 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1688 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1689 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1690 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1691 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 1692 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1693 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1694 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.771 \n",
      "Epoch: 1695 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1696 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1697 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1698 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1699 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1700 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.771 \n",
      "Epoch: 1701 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1702 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1703 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1704 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1705 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1706 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1707 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1708 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1709 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1710 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1711 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1712 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1713 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1714 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1715 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1716 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 1717 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1718 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1719 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 1720 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1721 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1722 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1723 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1724 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1725 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.770 \n",
      "Epoch: 1726 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1727 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 1728 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.773 \n",
      "Epoch: 1729 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1730 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1731 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1732 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1733 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1734 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1735 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1736 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1737 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1738 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1739 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1740 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1741 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1742 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 1743 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 1744 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1745 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1746 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1747 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.769 \n",
      "Epoch: 1748 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1749 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1750 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1751 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1752 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1753 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1754 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1755 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1756 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1757 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1758 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 1759 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1760 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.772 \n",
      "Epoch: 1761 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1762 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1763 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1764 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1765 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1766 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1767 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 1768 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1769 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1770 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.772 \n",
      "Epoch: 1771 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1772 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1773 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1774 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1775 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1776 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1777 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1778 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1779 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1780 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1781 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1782 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1783 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1784 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1785 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1786 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1787 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1788 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1789 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1790 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1791 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1792 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1793 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1794 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1795 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1796 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1797 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.770 \n",
      "Epoch: 1798 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1799 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1800 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1801 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1802 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1803 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1804 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.771 \n",
      "Epoch: 1805 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1806 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1807 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.767 \n",
      "Epoch: 1808 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1809 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1810 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1811 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1812 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1813 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1814 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1815 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1816 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1817 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1818 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1819 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1820 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1821 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.771 \n",
      "Epoch: 1822 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1823 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1824 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1825 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1826 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1827 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 1828 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1829 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1830 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1831 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 1832 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1833 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1834 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1835 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1836 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 1837 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1838 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1839 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1840 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1841 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1842 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1843 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1844 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1845 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1846 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1847 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1848 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1849 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1850 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1851 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1852 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1853 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1854 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1855 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1856 : Alpha Weight : 3.00000 | Test Acc : 77.20000 | Test Loss : 1.770 \n",
      "Epoch: 1857 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1858 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1859 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1860 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1861 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1862 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1863 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.771 \n",
      "Epoch: 1864 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 1865 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1866 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1867 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1868 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1869 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1870 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1871 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.767 \n",
      "Epoch: 1872 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1873 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1874 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1875 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1876 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1877 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1878 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1879 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1880 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1881 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1882 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 1883 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.767 \n",
      "Epoch: 1884 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1885 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1886 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1887 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.770 \n",
      "Epoch: 1888 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1889 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1890 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1891 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1892 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1893 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1894 : Alpha Weight : 3.00000 | Test Acc : 75.80000 | Test Loss : 1.772 \n",
      "Epoch: 1895 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1896 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1897 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1898 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1899 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1900 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.769 \n",
      "Epoch: 1901 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.772 \n",
      "Epoch: 1902 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1903 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1904 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1905 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1906 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.769 \n",
      "Epoch: 1907 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1908 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1909 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1910 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1911 : Alpha Weight : 3.00000 | Test Acc : 75.90000 | Test Loss : 1.770 \n",
      "Epoch: 1912 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1913 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1914 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1915 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1916 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1917 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1918 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 1919 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1920 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1921 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.769 \n",
      "Epoch: 1922 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1923 : Alpha Weight : 3.00000 | Test Acc : 77.10000 | Test Loss : 1.770 \n",
      "Epoch: 1924 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.768 \n",
      "Epoch: 1925 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1926 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.768 \n",
      "Epoch: 1927 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1928 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1929 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1930 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.771 \n",
      "Epoch: 1931 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1932 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1933 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1934 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1935 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.770 \n",
      "Epoch: 1936 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1937 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.772 \n",
      "Epoch: 1938 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1939 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1940 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1941 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1942 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1943 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.770 \n",
      "Epoch: 1944 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1945 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.768 \n",
      "Epoch: 1946 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.768 \n",
      "Epoch: 1947 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.771 \n",
      "Epoch: 1948 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1949 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1950 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.772 \n",
      "Epoch: 1951 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1952 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1953 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.772 \n",
      "Epoch: 1954 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1955 : Alpha Weight : 3.00000 | Test Acc : 77.00000 | Test Loss : 1.771 \n",
      "Epoch: 1956 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.772 \n",
      "Epoch: 1957 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.772 \n",
      "Epoch: 1958 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1959 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.772 \n",
      "Epoch: 1960 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.771 \n",
      "Epoch: 1961 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1962 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.771 \n",
      "Epoch: 1963 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1964 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.769 \n",
      "Epoch: 1965 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.770 \n",
      "Epoch: 1966 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1967 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.771 \n",
      "Epoch: 1968 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1969 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.768 \n",
      "Epoch: 1970 : Alpha Weight : 3.00000 | Test Acc : 76.70000 | Test Loss : 1.769 \n",
      "Epoch: 1971 : Alpha Weight : 3.00000 | Test Acc : 76.80000 | Test Loss : 1.768 \n",
      "Epoch: 1972 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1973 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1974 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1975 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1976 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1977 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1978 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n",
      "Epoch: 1979 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1980 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.769 \n",
      "Epoch: 1981 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.768 \n",
      "Epoch: 1982 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.771 \n",
      "Epoch: 1983 : Alpha Weight : 3.00000 | Test Acc : 76.00000 | Test Loss : 1.770 \n",
      "Epoch: 1984 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1985 : Alpha Weight : 3.00000 | Test Acc : 76.30000 | Test Loss : 1.769 \n",
      "Epoch: 1986 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1987 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1988 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.769 \n",
      "Epoch: 1989 : Alpha Weight : 3.00000 | Test Acc : 76.20000 | Test Loss : 1.770 \n",
      "Epoch: 1990 : Alpha Weight : 3.00000 | Test Acc : 76.90000 | Test Loss : 1.770 \n",
      "Epoch: 1991 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.769 \n",
      "Epoch: 1992 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.769 \n",
      "Epoch: 1993 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1994 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1995 : Alpha Weight : 3.00000 | Test Acc : 76.40000 | Test Loss : 1.770 \n",
      "Epoch: 1996 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.770 \n",
      "Epoch: 1997 : Alpha Weight : 3.00000 | Test Acc : 76.60000 | Test Loss : 1.770 \n",
      "Epoch: 1998 : Alpha Weight : 3.00000 | Test Acc : 76.10000 | Test Loss : 1.770 \n",
      "Epoch: 1999 : Alpha Weight : 3.00000 | Test Acc : 76.50000 | Test Loss : 1.771 \n"
     ]
    }
   ],
   "source": [
    "semisup_train(net,optimizer,criterion, train_dl, teach_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63b5e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'./resnet_size28_Model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38e84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def train_model(model, criterion, optimizer, num_epochs, train_loader,val_loader,teach_loader):\n",
    "#     since = time.time()\n",
    "#     best_model = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "#         # train\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "\n",
    "        \n",
    "#         for step, (inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "#             inputs = inputs.to(DEVICE)\n",
    "#             labels = labels.to(DEVICE)\n",
    "#             optimizer.zero_grad()\n",
    "#             with torch.set_grad_enabled(True):\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "#         epoch_loss = running_loss / train_size\n",
    "#         epoch_acc = running_corrects.double() / train_size\n",
    "#         print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        \n",
    "#         # validate\n",
    "#         model.eval()\n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs = inputs.to(DEVICE)\n",
    "#             labels = labels.to(DEVICE)\n",
    "#             optimizer.zero_grad()\n",
    "#             with torch.set_grad_enabled(False):\n",
    "#                 outputs = model(inputs)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "#                 loss = criterion(outputs, labels) # labeld dataset\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "#         epoch_loss = running_loss / val_size\n",
    "#         epoch_acc = running_corrects.double() / val_size\n",
    "#         print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "#         print('-' * 30)\n",
    "#         if epoch_acc > best_acc:\n",
    "#             best_acc = epoch_acc\n",
    "#             best_model = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best Val Acc: {:.4f}'.format(best_acc))\n",
    "#     model.load_state_dict(best_model)\n",
    "#     return model\n",
    "\n",
    "# # train the model\n",
    "# model = train_model(net, criterion, optimizer, NUM_EPOCHS, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6c34122",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_csv = pd.read_csv(os.path.join(base_path, \"test\",\"test_data.csv\"))\n",
    "\n",
    "ts_csv['path'] = ts_csv['file_name'].apply(\n",
    "    lambda x: os.path.join(test_path, x))\n",
    "\n",
    "test_x = ts_csv['path'].values\n",
    "test_ds = MnistDataset(test_x, transform = get_inferecne_transforms())\n",
    "\n",
    "test_dl = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1dd47601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e8632952c84b9b8aa96a8e3fbbc0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for step, input in tqdm(enumerate(test_dl), total=len(test_dl)):\n",
    "        input = input.to(DEVICE)\n",
    "        y_pred = net(input).detach().cpu().numpy().argmax(axis=1).astype(int)\n",
    "        \n",
    "        predictions.extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f3c1e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv') # sample submission 불러오기\n",
    "\n",
    "submission['label'] = predictions\n",
    "\n",
    "submission.to_csv('resnet_pseudo_argu_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel = np.concatenate([u.cpu().numpy() for u in teach_ds])\n",
    "pseudo_label = np.concatenate([u.cpu().numpy() for u in pseudo_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6642087",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tr_csv = pd.read_csv(os.path.join(base_path, \"train\",\"train_data.csv\"))\n",
    "y = x['label']\n",
    "x.drop(['label'], inplace = True, axis = 1)\n",
    "\n",
    "x = normalizer.transform(x.values) # Image.open(test_x[idx]\n",
    "\n",
    "tsne_x = np.concatenate([x, x_train, unlabel])\n",
    "tsne_y = np.concatenate([y.values, y_train, pseudo_label])\n",
    "\n",
    "embeddings = TSNE(perplexity = 30, n_jobs=-1, verbose = 1, n_iter = 500).fit_transform(tsne_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7bd88ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 50 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrows, ncols = 5, 10\n",
    "\n",
    "indices = np.random.randint(5000, size=(nrows, ncols))\n",
    "\n",
    "indices = np.random.randint(5000, size=(nrows, ncols))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, dpi=100, figsize=(5,5))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        idx = indices[i][j]\n",
    "        ax[i][j].imshow(Image.open(test_x[idx]), cmap='gray')\n",
    "        ax[i][j].set_title(f\"{predictions[idx]}\")\n",
    "        ax[i][j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94617c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
